import pandas as pd
import faiss
import numpy as np
import torch
from transformers import BertTokenizer, BertModel, pipeline
import logging
import warnings
import os
import gradio as gr

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Beta-Gamma tarzı uyarılar çıkıyordu araştırdığım kadarıyla bir sıkıntı yaratmıyor
warnings.filterwarnings("ignore", category=UserWarning, message=r".*beta.*")
warnings.filterwarnings("ignore", category=UserWarning, message=r".*gamma.*")

# diğer uyarıları bastırmak için araştırdığım kadarıyla sıkıntı olmuyor
loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
for logger in loggers:
    if "transformers" in logger.name.lower():
        logger.setLevel(logging.ERROR)

# BERT model ve tokenizer'ın yüklenmesi
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(
    model_name,
    use_safetensors=True,
    return_dict=False,
    attn_implementation="sdpa"
)

# BERT işlemlerini Cuda var ise onda çalıştırmak için yoksa cpu da çalışıyor
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# FAISS index'inin yüklenmesi
index = faiss.read_index('logs/faiss_index.index')

# Vektörlerin yüklendiği DataFrame'in yüklenmesi
log_df = pd.read_pickle('logs/bert_encoded_logs.pkl')

# Hugging Face'den LLM  yüklenmesi (FLAN-T5)
qa_pipeline = pipeline('text2text-generation', model='google/flan-t5-large')

# BERT ile vektörleştirme fonksiyonu
def bert_encode(texts, batch_size=32):
    all_embeddings = []
    model.eval()  # Modeli değerlendirme moduna alıyoruz (Gradient hesaplanmasın diye)
    with torch.no_grad():
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)
            outputs = model(**inputs)
            embeddings = outputs[0][:, 0, :].cpu().numpy()
            all_embeddings.extend(embeddings)
    return all_embeddings

# Sorgu işleme fonksiyonu
def query_data(query):
    # Sorgu metnini BERT ile vektörleştirme
    query_vector = bert_encode([query])[0].astype('float32')

    # FAISS index üzerinde sorgulama yapma
    k = 10  # kaç sonuç bulacağını yazıyoruz
    distances, indices = index.search(np.array([query_vector]), k)

    # Sonuçları işleme
    results = []
    for idx in indices[0]:
        results.append(log_df.iloc[idx]['Combined_Text'])

    # sonucu FAISS ile döndürme
    faiss_output = "\n\n".join(results)

    # LLM ile FAISS sonucu üzerinde daha ileri işlem yapma
    context = "\n".join(results)
    prompt = f"Based on the following logs, answer the query: '{query}'. Please provide a detailed answer and include an explanation.\n\n{context}"
    llm_output = qa_pipeline(prompt)[0]['generated_text']

    return faiss_output, llm_output

# Gradio arayüzü oluşturma Arayüz üzerinde ingilizce kullandım çünkü sorguların da ingilizce olması daha iyi sonuç vereceğini umuyorum
with gr.Blocks(theme=gr.themes.Base(), title="LogGpt") as demo:
    gr.Markdown(
        """
        # LOGGPT
        """)
    textbox = gr.Textbox(label="Enter your Question:")
    with gr.Row():
        button = gr.Button("Submit", variant="primary")
    with gr.Column():
        output1 = gr.Textbox(lines=10, max_lines=20, label="Output with just FAISS Vector Search (returns text field as is):")
        output2 = gr.Textbox(lines=10, max_lines=20, label="Output generated by chaining FAISS Vector Search to Langchain's RetrieverQA + HuggingFace LLM:")

    button.click(query_data, textbox, outputs=[output1, output2])

demo.launch()
